import com.google.cloud.storage.Storage.{BlobListOption, CopyRequest}
import com.google.cloud.storage._
import com.google.cloud._
import org.apache.spark.sql.SparkSession
import scala.collection.JavaConverters.iterableAsScalaIterableConverter
val spark = SparkSession.builder()
      .master("local[1]")
      .appName("SparkByExample")
      .getOrCreate();
lazy val service: Storage = StorageOptions.getDefaultInstance.getService
val in = service.list("shivaya_nama", Storage.BlobListOption.prefix("reyan")).iterateAll().asScala.toList
val j = in.map(x => (x.getBucket , x.getName ,  x.getGeneration , x.getSize , x.getContentType)).collect{case (bucket,name,genertion,size,contenttype) => (bucket,name,genertion,size,contenttype)}.toDF("Bucket_name","File_name","date","File_size","File_type")
val f = j.select(col("Bucket_name"),col("File_name"),col("date"),substring(col("date"),1,10).as ("UTC_date"),col("File_size"),col("File_type")).createOrReplaceTempView("h")
val hj = spark.sql("select h.Bucket_name,h.File_name,h.date,h.File_size,h.File_type,from_unixtime(UTC_date)  as `Utc` from h").createOrReplaceTempView("k")
val b = spark.sql("SELECT  k.Bucket_name,k.File_name,k.date,k.File_size,k.File_type,k.Utc,from_utc_timestamp(k.Utc, 'UTC+5:30') as Ist from k").createOrReplaceTempView("f")
val out = spark.sql("select f.Bucket_name,f.File_name,f.File_size,f.File_type,substring(f.Ist,1,10) as IST_date,substring(f.Ist,12,10) as IST_time,substring(f.UTC,1,10) as UTC_date,substring(f.UTC,12,10) as UTC_time from f").toDF()
val op= out.coalesce(1).write.mode("append").format("csv").option("header","true").save("gs://shivaya_nama//op1")
