import com.google.cloud.storage.Storage.{BlobListOption, CopyRequest}
import com.google.cloud.storage._
import com.google.cloud._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions
import org.apache.spark.sql.Column
import scala.collection.JavaConverters.iterableAsScalaIterableConverter
val spark = SparkSession.builder()
      .master("local[1]")
      .appName("SparkByExample")
      .getOrCreate();lazy val service: Storage = StorageOptions.getDefaultInstance.getService
      lazy val service: Storage = StorageOptions.getDefaultInstance.getService
val fetchFileInfo = service.list("shivaya_nama", Storage.BlobListOption.prefix("PCD")).iterateAll().asScala.toList
  
val df = fetchFileInfo
    .map(x => (x.getBucket , x.getName ,  x.getGeneration , x.getSize , x.getContentType))
    .collect{case (bucket,name,genertion,size,contenttype) => (bucket,name,genertion,size,contenttype)}
    .toDF("Bucket_Name","File_Name","Date","File_Size","File_Type")
    .select(col("Bucket_Name"),col("File_name"),from_unixtime(substring(col("Date"), 1, 10)).as("Date1"),col("File_Size"),col("File_Type"))
    .filter(col("Date1")===current_date())
    
 
 df.createOrReplaceTempView("k")

val fileTuples_final = spark.sql("SELECT  k.Bucket_name,substring(k.File_name,5,17) as File_Pattern,k.File_size,k.File_type,substring(k.Date1,1,10) as date,substring(k.Date1,12,10) as UTC_time,substring(from_utc_timestamp(k.Date1, 'UTC+5:30'),12,10)as IST_Time from k ").toDF()
val op=fileTuples_final.coalesce(1).write.mode("append").format("csv").option("header","true").save("gs://shivaya_nama//Final_output")
